{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pickle\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "from collections import Counter\n",
    "from scipy.stats import mode\n",
    "import pandas as pd\n",
    "\n",
    "from dataset_utils.text_processing import tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## store implications in imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dir = 'data/imdb/'\n",
    "out_dir = 'data/imdb_imps/'\n",
    "file = 'minival'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imps = pickle.load(open('data/imdb_imps/vqa_'+file+'_imps.pkl','rb'))\n",
    "imdb = numpy.load(in_dir+'imdb_'+file+'2014.npy',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_map = {\n",
    " 'ans=0 implies none' : 'logeq',\n",
    " 'ans>0 implies some': 'necessary_condition',\n",
    " 'color mutex': 'mutex',\n",
    " 'color_in_answer_must_be_in_picture': 'necessary_condition',\n",
    " 'n+1': 'mutex',\n",
    " 'noun_in_answer_must_be_in_picture': 'necessary_condition',\n",
    " 'remove_modifier': 'necessary_condition',\n",
    " 'subjectyes': 'logeq',\n",
    " 'what': 'logeq',\n",
    " 'where': 'logeq',\n",
    " 'whereprep': 'logeq',\n",
    " 'wordnet mutex': 'mutex',\n",
    " 'wordnet_adj_mutex': 'mutex',\n",
    " 'xory_no': 'mutex',\n",
    " 'xory_yes': 'logeq',\n",
    " 'yeseqcount': 'logeq'\n",
    "}\n",
    "\n",
    "mp = {\n",
    "  'logeq':[1,0,0],\n",
    "  'necessary_condition':[0,1,0],\n",
    "  'mutex':[0,0,1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in imdb[1:]:\n",
    "    key = i['question_id']\n",
    "    \n",
    "    if 0 in [len(v) for v in imps[key].values()]: # if any valid answer doesn't have any implications  \n",
    "        i['is_imps'] = False\n",
    "    else:\n",
    "        i['is_imps'] = True\n",
    "        i['qa_implications'] = imps[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in imdb[1:]:\n",
    "    if i['is_imps']:\n",
    "        qa = i['qa_implications']\n",
    "        i['qa_tokens']={}\n",
    "        i['qa_answers']={}\n",
    "        i['imp_type']={}\n",
    "        for key in qa.keys():\n",
    "            i['qa_tokens'][key] = []\n",
    "            i['qa_answers'][key] = []\n",
    "            i['imp_type'][key] = []\n",
    "            \n",
    "            for imp in qa[key]:\n",
    "                i['qa_tokens'][key].append(tokenize(imp[0]))\n",
    "                i['qa_answers'][key].append(imp[1])\n",
    "                i['imp_type'][key].append(mp[source_map[imp[2]]])\n",
    "\n",
    "        i.pop('qa_implications',None)\n",
    "    \n",
    "    else:\n",
    "        i['qa_tokens']={}\n",
    "        i['qa_answers']={}\n",
    "        for key in set(i['valid_answers']):\n",
    "            i['qa_tokens'][key] = [i['question_tokens']]\n",
    "            i['qa_answers'][key] = [key]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# imp_flag =false if there were only necc implications for some valid answers!\n",
    "\n",
    "count = 0\n",
    "for i in imdb[1:]:\n",
    "    if i['is_imps']:\n",
    "        if 0 in [len(v) for v in i['imp_type'].values()]:\n",
    "            i['is_imps'] = False\n",
    "            i.pop('imp_type',None)\n",
    "            \n",
    "            i['qa_tokens']={}\n",
    "            i['qa_answers']={}\n",
    "            for key in set(i['valid_answers']):\n",
    "                i['qa_tokens'][key] = [i['question_tokens']]\n",
    "                i['qa_answers'][key] = [key]\n",
    "            \n",
    "            count+=1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "count/len(imdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(imdb,open(out_dir+'imdb_'+file+'2014.npy','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Ons for several restrictions in imdb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = numpy.load(out_dir+'imdb_'+file+'2014.npy',allow_pickle=True)\n",
    "imdb_ori = numpy.load(in_dir+'imdb_'+file+'2014.npy',allow_pickle=True)\n",
    "q = json.load(open('orig_data/vqa_v2.0/v2_mscoco_'+file+'2014_annotations.json','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and delete previous keys\n",
    "\n",
    "for i in imdb[1:]:\n",
    "    if i['is_imps']:\n",
    "        qa = i['qa_implications']\n",
    "        i['qa_tokens']={}\n",
    "        i['qa_answers']={}\n",
    "        for key in qa.keys():\n",
    "            i['qa_tokens'][key] = []\n",
    "            i['qa_answers'][key] = []\n",
    "            for imp in qa[key]:\n",
    "                i['qa_tokens'][key].append(text_processing.tokenize(imp[0]))\n",
    "                i['qa_answers'][key].append(imp[1])\n",
    "        i.pop('qa_implications',None)\n",
    "    \n",
    "    else:\n",
    "        i['qa_tokens']={}\n",
    "        i['qa_answers']={}\n",
    "        for key in set(i['valid_answers']):\n",
    "            i['qa_tokens'][key] = [i['question_tokens']]\n",
    "            i['qa_answers'][key] = [key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mode\n",
    "\n",
    "for i in imdb[1:]:\n",
    "    if not i['is_imps']:\n",
    "        for key in i['qa_answers']:\n",
    "            if key not in ['yes','no']:\n",
    "                i['qa_answers'][key] = [mode(i['valid_answers'])[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qmap = {}\n",
    "\n",
    "for ann in q['annotations']:\n",
    "    atype = ann['answer_type']\n",
    "    qid = ann['question_id']\n",
    "    qmap[qid] = atype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = []\n",
    "for i in range(1,len(imdb)):\n",
    "    if not imdb[i]['is_imps'] and qmap[imdb[i]['question_id']] != 'yes/no':\n",
    "        idx.append(i)\n",
    "\n",
    "imdb = numpy.delete(imdb,idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in imdb[1:]:\n",
    "    if not i['is_imps'] and qmap[i['question_id']]!='yes/no':\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete questions w/o any implications for all valid answers\n",
    "\n",
    "idx = []\n",
    "for i in range(1,len(imdb)):\n",
    "    if not imdb[i]['is_imps']:\n",
    "        idx.append(i)\n",
    "    imdb[i].pop('is_imps')\n",
    "\n",
    "imdb = numpy.delete(imdb,idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete questions w/o implications for any valid answers\n",
    "\n",
    "idx = []\n",
    "for i in range(1,len(imdb)):\n",
    "    qa = imdb[i]['qa_answers']\n",
    "    for key in qa.keys():\n",
    "        if len(qa[key])==0 :\n",
    "            idx.append(i)\n",
    "            break\n",
    "\n",
    "imdb = numpy.delete(imdb,idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(imdb,open(out_dir+'imdb_'+file+'2014.npy','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dir = 'data/imdb/'\n",
    "out_dir = 'data/imdb_imps/'\n",
    "file = 'train'\n",
    "\n",
    "imdb = numpy.load(out_dir+'imdb_'+file+'2014.npy',allow_pickle=True)\n",
    "imdb_ori = numpy.load(in_dir+'imdb_'+file+'2014.npy',allow_pickle=True)\n",
    "q = json.load(open('orig_data/vqa_v2.0/v2_mscoco_'+file+'2014_annotations.json','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_types = {}\n",
    "qmap = {}\n",
    "\n",
    "for ann in q['annotations']:\n",
    "    atype = ann['answer_type']\n",
    "    qid = ann['question_id']\n",
    "    qmap[qid] = atype\n",
    "    if atype not in question_types.keys():\n",
    "        question_types[atype] = []\n",
    "    question_types[atype].append(qid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Stats original vqa2.0')\n",
    "print('Total number of questions: %d' %(len(q['annotations'])))\n",
    "for key in question_types.keys():\n",
    "    print('%s lenght: %d percentage: %.2f' % (key,len(question_types[key]),100*len(question_types[key])/len(q['annotations'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_question_types = {}\n",
    "\n",
    "for ann in imdb[1:]:\n",
    "    qid = ann['question_id']\n",
    "    atype = qmap[qid]\n",
    "    \n",
    "    if atype not in updated_question_types.keys():\n",
    "        updated_question_types[atype]=[]\n",
    "    updated_question_types[atype].append(qid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Stats new dataset:')\n",
    "for key in updated_question_types:\n",
    "    print('%s lenght: %d percentage: %.2f' % (key,len(updated_question_types[key]),100*len(updated_question_types[key])/len(imdb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files = ['orig_data/vqa_v2.0/v2_OpenEnded_mscoco_train2014_questions.json',\n",
    "                'orig_data/vqa_v2.0/v2_OpenEnded_mscoco_val2014_questions.json',\n",
    "                'orig_data/vqa_v2.0/v2_OpenEnded_mscoco_test2015_questions.json']\n",
    "out_dir = '../'\n",
    "min_freq = 0\n",
    "vocab_file_name = 'vocabulary_vqa.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = Counter()\n",
    "questions = []\n",
    "\n",
    "for idx, input_file in enumerate(input_files):\n",
    "    with open(input_file, 'r') as f:\n",
    "        questions += json.load(f)['questions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_length = [None]*len(questions)\n",
    "for inx, question in enumerate(questions):\n",
    "    words = tokenize(question['question'])\n",
    "    question_length[inx] = len(words)\n",
    "    word_count.update(words)\n",
    "\n",
    "vocabulary = [w[0] for w in word_count.items() if w[1] >= min_freq]\n",
    "vocabulary.sort()\n",
    "vocabulary = ['<unk>'] + vocabulary\n",
    "\n",
    "len(vocabulary) #from original dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['train','val2train','minival'] #from implications\n",
    "questions_imps = []\n",
    "\n",
    "for file in files:\n",
    "    imps = pickle.load(open('data/imdb_imps/vqa_'+file+'_imps.pkl','rb'))\n",
    "    imdb_ori = numpy.load('data/imdb/imdb_'+file+'2014.npy',allow_pickle=True)\n",
    "\n",
    "    for i in imdb_ori[1:]:\n",
    "        key = i['question_id']\n",
    "        questions_imps += [q[0] for v in imps[key].values() for q in v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in questions_imps:\n",
    "    words = tokenize(question)\n",
    "    word_count.update(words)\n",
    "    \n",
    "vocabulary = [w[0] for w in word_count.items() if w[1] >= min_freq]\n",
    "vocabulary.sort()\n",
    "vocabulary = ['<unk>'] + vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(questions_imps),len(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = os.path.join(out_dir, vocab_file_name)\n",
    "with open(vocab_file, 'w') as f:\n",
    "    f.writelines([w+'\\n' for w in vocabulary])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create implications imdb (for augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dir = 'data/imdb/'\n",
    "out_dir = 'data/imdb_imps/'\n",
    "file = 'val2train'\n",
    "\n",
    "imps = pickle.load(open(out_dir+'vqa_'+file+'_imps.pkl','rb'))\n",
    "imdb = numpy.load(out_dir+'imdb_'+file+'2014.npy',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_just_imps = [imdb[0].copy()]\n",
    "\n",
    "for i in imdb[1:]:\n",
    "    \n",
    "    if i['is_imps']:\n",
    "        vans = mode(i['valid_answers'])[0][0]\n",
    "\n",
    "        for q,a,implied in zip(i['qa_tokens'][vans],i['qa_answers'][vans],imps[i['question_id']][vans]):\n",
    "            cp = i.copy()\n",
    "            cp.pop('qa_tokens',None)\n",
    "            cp.pop('qa_answers',None)\n",
    "            cp.pop('is_imps',None)\n",
    "            cp['question_str'] = implied[0]\n",
    "            cp['question_tokens'] = q\n",
    "            cp['valid_answers'] = [a for _ in range(10)]\n",
    "            cp['all_answers'] = [a for _ in range(10)]\n",
    "            imdb_just_imps.append(cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(imdb_just_imps,open(in_dir+'imdb_just_imps'+file+'2014.npy','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(imdb_just_imps),len(imdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_just_imps[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data for manual annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dir = 'data/imdb/'\n",
    "imdb_v = numpy.load(in_dir+'imdb_val2014.npy',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_v = json.load(open('orig_data/vqa_v2.0/v2_mscoco_val2014_annotations.json','rb'))\n",
    "\n",
    "qmap = {}\n",
    "    \n",
    "for ann in q_v['annotations']:\n",
    "    atype = ann['answer_type']\n",
    "    qid = ann['question_id']\n",
    "    qmap[qid] = atype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_set = set()\n",
    "for i in imdb_man[1:]:\n",
    "    _set.add(i['question_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = []\n",
    "for i in imdb_v[1:]:\n",
    "    if qmap[i['question_id']] !='yes/no' and i['question_id'] not in _set:\n",
    "        imdb.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "sel = random.sample(imdb,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for i in sel:\n",
    "    qid = i['question_id']\n",
    "    q = i['question_str']\n",
    "    a = mode(i['valid_answers'])[0][0]\n",
    "    if a not in ['unknown','<unk>']:\n",
    "        data.append({'qid':qid,'question':q,'answer':a})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "df['Logeq'] = \"\"\n",
    "df['Necc'] = \"\"\n",
    "df['Mutex'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direc = 'manualAnnotations_new/'\n",
    "for day in numpy.arange(8):\n",
    "    i=1200*day\n",
    "    df[i:400+i].to_excel(direc+str(day+1)+'_1.xlsx')\n",
    "    df[400+i:800+i].to_excel(direc+str(day+1)+'_2.xlsx')\n",
    "    df[800+i:1200+i].to_excel(direc+str(day+1)+'_3.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(direc+'all.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## manual to imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_list = []\n",
    "for i in range(1,5):\n",
    "\n",
    "    _1 = pd.read_excel('manualAnnotations/day{}_1.xlsx'.format(i),index_col=0)\n",
    "    _1.dropna(inplace=True)\n",
    "    _2 = pd.read_excel('manualAnnotations/day{}_2.xlsx'.format(i),index_col=0)\n",
    "    _2.dropna(inplace=True)\n",
    "    _3 = pd.read_excel('manualAnnotations/day{}_3.xlsx'.format(i),index_col=0)\n",
    "    _3.dropna(inplace=True)\n",
    "    _list.extend([_1,_2,_3])\n",
    "data = pd.concat(_list,sort = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_val = numpy.load('data/imdb/imdb_val2014.npy',allow_pickle=True)\n",
    "imdb_train = numpy.load('data/imdb/imdb_train2014.npy',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_set = set()\n",
    "for i in imdb_train[1:]:\n",
    "    _set.add(i['question_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_man = [imdb_val[0].copy()]\n",
    "\n",
    "for _,d in data.iterrows():\n",
    "    entry = {}\n",
    "    qid = d['qid']\n",
    "    if qid not in _set:\n",
    "        continue\n",
    "    image_id = int(qid/1000)\n",
    "    \n",
    "    entry['image_name'] = 'COCO_train2014_'+str(image_id).zfill(12)\n",
    "    entry['image_id'] = image_id\n",
    "    entry['question_id'] = qid\n",
    "    entry['feature_path'] = 'COCO_train2014_'+str(image_id).zfill(12)+'.npy'\n",
    "    \n",
    "    for q, a in zip(['Logeq','Necc','Mutex'],['yes','yes','no']):\n",
    "        entry['question_str'] = d[q]\n",
    "        entry['question_tokens'] = tokenize(d[q])\n",
    "        entry['valid_answers'] = [a for _ in range(10)]\n",
    "        entry['all_answers'] = [a for _ in range(10)]\n",
    "        imdb_man.append(entry.copy()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(imdb_man,open('data/imdb_manual/imdb_train2014.npy','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(imdb_man)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_val = numpy.load('data/imdb/imdb_val2014.npy',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4426"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c=0\n",
    "for i in imdb_val[1:]:\n",
    "    a = mode(i['valid_answers'])[0][0]\n",
    "    if a in ['unknown','<unk>']:\n",
    "        c+=1\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02064799048307714"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c/len(imdb_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
