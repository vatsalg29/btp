{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = 'orig_data/vqa_v2.0/v2_OpenEnded_mscoco_test2015_questions.json'\n",
    "#p2 = '/home/btp/pg_aa_1/pythia/test_best_model.json'\n",
    "p2 = '/home1/BTP/pg_aa_1/btp/results/pythia_cycle_consistent/29/best_model_predict_test.json'\n",
    "\n",
    "res = json.load(open(p2,'r'))\n",
    "ques = json.load(open(p1,'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qqa = {}\n",
    "for q in ques['questions']:\n",
    "    qqa[q['question_id']] = q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = random.choices(range(0,len(res)),k=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in indices:\n",
    "    r = res[i]\n",
    "    q_id = r['question_id']\n",
    "    ans = r['answer']\n",
    "    question = qqa[q_id]['question']\n",
    "    imgId = qqa[q_id]['image_id']\n",
    "    imgFilename = 'orig_data/vqa_v2.0/test2015/' + 'COCO_test2015_'+ str(imgId).zfill(12) + '.jpg'\n",
    "    \n",
    "    if os.path.isfile(imgFilename):\n",
    "        I = io.imread(imgFilename)\n",
    "        plt.imshow(I)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    print(question)\n",
    "    print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generated Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "gq_1 = numpy.load(\"/home1/BTP/pg_aa_1/btp/boards/pythia_cycle_consistent/3012/gq_25000.npy\", allow_pickle=True)\n",
    "gq = gq_1.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['how',\n",
       " 'many',\n",
       " 'sources',\n",
       " 'of',\n",
       " 'light',\n",
       " 'are',\n",
       " 'there',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "indices = random.choices(range(len(gq['annotations'])), k=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'indices' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-b3f3895c659a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# for i in range(len(gq['annotations'])):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mentry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'annotations'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mia\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'imp_ans'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ques_answers'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'orig_ans'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'indices' is not defined"
     ]
    }
   ],
   "source": [
    "for i in indices :\n",
    "# for i in range(len(gq['annotations'])):\n",
    "    entry = gq['annotations'][i]\n",
    "    ia = entry['imp_ans']\n",
    "    oa = gq['ques_answers'][i]['orig_ans']\n",
    "    if oa == ia:\n",
    "        continue\n",
    "    q = entry['gen_ques']\n",
    "    q = q.replace('<start>', '')\n",
    "    q = q.replace('<unk>', '')\n",
    "    q = q.replace('<end>', '')\n",
    "    q = q.strip()\n",
    "    oq = gq['ques_answers'][i]['orig_ques']\n",
    "    if '<end>' in oq:\n",
    "        oq = oq[:oq.index(\"<end>\")]\n",
    "    oq = oq.replace('<start>', '')\n",
    "    oq = oq.replace('<unk>', '')\n",
    "    oq = oq.replace('<end>', '')\n",
    "    oq = oq.strip()\n",
    "    if(len(oq.split()) > 4):\n",
    "        continue\n",
    "    qgt = entry['gt_gen_ques']\n",
    "#     if '<end>' in qgt:\n",
    "#         qgt = qgt[:qgt.index(\"<end>\")]\n",
    "    qgt = qgt.replace('<start>', '')\n",
    "    qgt = qgt.replace('<unk>', '')\n",
    "    qgt = qgt.replace('<end>', '')\n",
    "    qgt = qgt.strip()\n",
    "    imgId = entry['image_id']\n",
    "    imgFilename = 'orig_data/vqa_v2.0/val2014/' + 'COCO_val2014_'+ str(imgId).zfill(12) + '.jpg'\n",
    "    \n",
    "    if os.path.isfile(imgFilename):\n",
    "        I = io.imread(imgFilename)\n",
    "        plt.imshow(I)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    print(\"Original ques : \" + oq)\n",
    "    print(\"Original Answer : \" + oa)\n",
    "    print(\"Implied Answer : \" + ia)\n",
    "    print(\"Generated ques : \" + q)\n",
    "    print(\"Ground Truth Implication : \" + qgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The bleu-1 score is: 0.7127741304049194\n",
      "The bleu-2 score is: 0.6402352375461843\n",
      "The bleu-3 score is: 0.5403273364643447\n",
      "The bleu-4 score is: 0.40583196873568644\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "score_1 = 0\n",
    "score_2 = 0\n",
    "score_3 = 0\n",
    "score_4 = 0\n",
    "count = 0\n",
    "for i in range(len(gq['annotations'])):\n",
    "    oa = gq['ques_answers'][i]['orig_ans']\n",
    "    entry = gq['annotations'][i]\n",
    "    ia = entry['imp_ans']\n",
    "    if oa == 'yes' or oa == 'no':\n",
    "        continue\n",
    "    if ia != 'yes' and ia != 'no':\n",
    "        continue\n",
    "    q = entry['gen_ques']\n",
    "    if '<end>' in q:\n",
    "        q = q[:q.index(\"<end>\")]\n",
    "    q = q.replace('<start>', '')\n",
    "    q = q.replace('<unk>', '')\n",
    "    q = q.replace('<end>', '')\n",
    "    q = q.replace('!', '')\n",
    "    q = q.strip()\n",
    "    qgt = entry['gt_gen_ques']\n",
    "    qgt = qgt.replace('<start>', '')\n",
    "    qgt = qgt.replace('<unk>', '')\n",
    "    qgt = qgt.replace('<end>', '')\n",
    "    qgt = qgt.replace('!', '')\n",
    "    qgt = qgt.strip()\n",
    "    score_1 += sentence_bleu([qgt.split()], q.split(), weights=(1,0,0,0))\n",
    "    score_2 += sentence_bleu([qgt.split()], q.split(), weights=(0.5,0.5,0,0))\n",
    "    score_3 += sentence_bleu([qgt.split()], q.split(), weights=(0.33,0.33,0.33,0))\n",
    "    score_4 += sentence_bleu([qgt.split()], q.split(), weights=(0.25,0.25,0.25,0.25))\n",
    "    count += 1\n",
    "    \n",
    "score_1 /= count\n",
    "score_2 /= count\n",
    "score_3 /= count\n",
    "score_4 /= count\n",
    "print(\"The bleu-1 score is: \"+str(score_1))\n",
    "print(\"The bleu-2 score is: \"+str(score_2))\n",
    "print(\"The bleu-3 score is: \"+str(score_3))\n",
    "print(\"The bleu-4 score is: \"+str(score_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "gq_gt_1 = numpy.load(\"/home1/BTP/pg_aa_1/btp/boards/pythia_cycle_consistent/3009/gq_25000.npy\", allow_pickle=True)\n",
    "gq_gt = gq_gt_1.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = gq_gt['annotations'][11]['gen_ques']\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The bleu-1 score is: 0.753129973729357\n",
      "The bleu-2 score is: 0.6905358886634695\n",
      "The bleu-3 score is: 0.6060766515607181\n",
      "The bleu-4 score is: 0.48953548617353654\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "score_1 = 0\n",
    "score_2 = 0\n",
    "score_3 = 0\n",
    "score_4 = 0\n",
    "count = 0\n",
    "for i in range(len(gq_gt['annotations'])):\n",
    "    oa = gq_gt['ques_answers'][i]['orig_ans']\n",
    "    entry = gq_gt['annotations'][i]\n",
    "    ia = entry['imp_ans']\n",
    "    if oa == 'yes' or oa == 'no':\n",
    "        continue\n",
    "    if ia != 'yes' and ia != 'no':\n",
    "        continue\n",
    "    q = entry['gen_ques']\n",
    "    if '<end>' in q:\n",
    "        q = q[:q.index(\"<end>\")]\n",
    "    q = q.replace('<start>', '')\n",
    "    q = q.replace('<unk>', '')\n",
    "    q = q.replace('<end>', '')\n",
    "    q = q.replace('!', '')\n",
    "    q = q.strip()\n",
    "    qgt = entry['gt_gen_ques']\n",
    "    qgt = qgt.replace('<start>', '')\n",
    "    qgt = qgt.replace('<unk>', '')\n",
    "    qgt = qgt.replace('<end>', '')\n",
    "    qgt = qgt.replace('!', '')\n",
    "    qgt = qgt.strip()\n",
    "    score_1 += sentence_bleu([qgt.split()], q.split(), weights=(1,0,0,0))\n",
    "    score_2 += sentence_bleu([qgt.split()], q.split(), weights=(0.5,0.5,0,0))\n",
    "    score_3 += sentence_bleu([qgt.split()], q.split(), weights=(0.33,0.33,0.33,0))\n",
    "    score_4 += sentence_bleu([qgt.split()], q.split(), weights=(0.25,0.25,0.25,0.25))\n",
    "    count += 1\n",
    "    \n",
    "score_1 /= count\n",
    "score_2 /= count\n",
    "score_3 /= count\n",
    "score_4 /= count\n",
    "print(\"The bleu-1 score is: \"+str(score_1))\n",
    "print(\"The bleu-2 score is: \"+str(score_2))\n",
    "print(\"The bleu-3 score is: \"+str(score_3))\n",
    "print(\"The bleu-4 score is: \"+str(score_4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final score calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = np.load(\"/home1/BTP/pg_aa_1/btp/data/imdb_manual/imdb_val2014.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_name': 'COCO_val2014_000000405135',\n",
       " 'image_id': 405135,\n",
       " 'feature_path': 'COCO_val2014_000000405135.npy',\n",
       " 'question_id': 4051350011,\n",
       " 'question_str': ' is the horse brown?',\n",
       " 'question_tokens': ['is', 'the', 'horse', 'brown'],\n",
       " 'valid_answers': ['yes',\n",
       "  'yes',\n",
       "  'yes',\n",
       "  'yes',\n",
       "  'yes',\n",
       "  'yes',\n",
       "  'yes',\n",
       "  'yes',\n",
       "  'yes',\n",
       "  'yes'],\n",
       " 'all_answers': ['yes',\n",
       "  'yes',\n",
       "  'yes',\n",
       "  'yes',\n",
       "  'yes',\n",
       "  'yes',\n",
       "  'yes',\n",
       "  'yes',\n",
       "  'yes',\n",
       "  'yes']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "gq_1 = np.load(\"/home1/BTP/pg_aa_1/btp/boards/pythia_cycle_consistent/3000/gq_70000.npy\", allow_pickle=True)\n",
    "gq_2 = np.load(\"/home1/BTP/pg_aa_1/btp/boards/pythia_cycle_consistent/3000/gq_70001.npy\", allow_pickle=True)\n",
    "gq_3 = np.load(\"/home1/BTP/pg_aa_1/btp/boards/pythia_cycle_consistent/3000/gq_70002.npy\", allow_pickle=True)\n",
    "gq_1 = gq_1.item()\n",
    "gq_2 = gq_2.item()\n",
    "gq_3 = gq_3.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = {}\n",
    "for i in range(1,len(imdb)):\n",
    "    m[imdb[i]['question_id']] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(gq_1['annotations'])):\n",
    "    assert gq_3['annotations'][i]['ques_id'] == imdb[i+1]['question_id']\n",
    "    assert gq_1['ques_answers'][i]['orig_ans'] == gq_2['ques_answers'][i]['orig_ans']\n",
    "    assert gq_2['ques_answers'][i]['orig_ans'] == gq_3['ques_answers'][i]['orig_ans']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, array([0.]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pycocoevalcap.cider.cider_scorer import CiderScorer\n",
    "qgtr = \" \".join(qgt)\n",
    "cider_scorer = CiderScorer()\n",
    "cider_scorer += (q, qgt)\n",
    "cider_scorer.compute_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count: 255682\n",
      "The bleu-1 score is: 0.5983495169006039\n",
      "The bleu-2 score is: 0.473781651150831\n",
      "The bleu-3 score is: 0.37042470135893196\n",
      "The bleu-4 score is: 0.2608719133234902\n",
      "The ROUGE-L score is: 0.6306325372119695\n",
      "The METEOR score is: 0.5675918578871159\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.meteor_score import single_meteor_score\n",
    "from rouge import Rouge\n",
    "from pycocoevalcap.cider.cider_scorer import CiderScorer\n",
    "score_1 = 0\n",
    "score_2 = 0\n",
    "score_3 = 0\n",
    "score_4 = 0\n",
    "score_r = 0\n",
    "score_m = 0\n",
    "count = 0\n",
    "for i in range(1, len(imdb)):\n",
    "    if not imdb[i]['is_imps']:\n",
    "        continue\n",
    "    chosen_ans = gq_1['ques_answers'][i-1]['orig_ans']\n",
    "    num_imps = len(imdb[i]['imp_type'][chosen_ans])\n",
    "    for j in range(num_imps):\n",
    "        imp_type = np.argmax(imdb[i]['imp_type'][chosen_ans][j]) + 1\n",
    "        if imp_type == 1:\n",
    "            gq = gq_1\n",
    "        elif imp_type == 2:\n",
    "            gq = gq_2\n",
    "        else:\n",
    "            gq = gq_3\n",
    "        assert gq['annotations'][i-1]['ques_id'] == imdb[i]['question_id']\n",
    "        q = gq['annotations'][i-1]['gen_ques']\n",
    "        if '<end>' in q:\n",
    "            q = q[:q.index(\"<end>\")]\n",
    "        q = q.replace('<start>', '')\n",
    "        q = q.replace('<unk>', '')\n",
    "        q = q.replace('<end>', '')\n",
    "        q = q.strip()\n",
    "        qgt = imdb[i]['qa_tokens'][chosen_ans][j]\n",
    "        rouge = Rouge()\n",
    "        cider_scorer = CiderScorer()\n",
    "        qgtr = \" \".join(qgt)\n",
    "        score_1 += sentence_bleu([qgt], q.split(), weights=(1,0,0,0))\n",
    "        score_2 += sentence_bleu([qgt], q.split(), weights=(0.5,0.5,0,0))\n",
    "        score_3 += sentence_bleu([qgt], q.split(), weights=(0.33,0.33,0.33,0))\n",
    "        score_4 += sentence_bleu([qgt], q.split(), weights=(0.25,0.25,0.25,0.25))\n",
    "        score_m += single_meteor_score(qgtr, q)\n",
    "        score_r += rouge.get_scores(q, qgtr)[0]['rouge-l']['f']\n",
    "        cider_scorer += (q, qgt)\n",
    "        count += 1\n",
    "    \n",
    "score_1 /= count\n",
    "score_2 /= count\n",
    "score_3 /= count\n",
    "score_4 /= count\n",
    "score_m /= count\n",
    "score_r /= count\n",
    "print(\"Count: \"+str(count))\n",
    "print(\"The bleu-1 score is: \"+str(score_1))\n",
    "print(\"The bleu-2 score is: \"+str(score_2))\n",
    "print(\"The bleu-3 score is: \"+str(score_3))\n",
    "print(\"The bleu-4 score is: \"+str(score_4))\n",
    "print(\"The ROUGE-L score is: \"+str(score_r))\n",
    "print(\"The METEOR score is: \"+str(score_m))          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, array([0.]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cider_scorer.compute_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
