{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pickle\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "from collections import Counter\n",
    "from scipy.stats import mode\n",
    "\n",
    "\n",
    "from dataset_utils.text_processing import tokenize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## store implications in imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dir = 'data/imdb/'\n",
    "out_dir = 'data/imdb_imps/'\n",
    "file = 'minival'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imps = pickle.load(open(out_dir+'vqa_'+file+'_imps.pkl','rb'))\n",
    "imdb = numpy.load(in_dir+'imdb_'+file+'2014.npy',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in imdb[1:]:\n",
    "    key = i['question_id']\n",
    "    if 0 in [len(v) for v in imps[key].values()]: # if any valid answer doesn't have any implications  \n",
    "        i['is_imps'] = False\n",
    "    else:\n",
    "        i['is_imps'] = True\n",
    "        i['qa_implications'] = imps[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in imdb[1:]:\n",
    "    if i['is_imps']:\n",
    "        qa = i['qa_implications']\n",
    "        i['qa_tokens']={}\n",
    "        i['qa_answers']={}\n",
    "        for key in qa.keys():\n",
    "            i['qa_tokens'][key] = []\n",
    "            i['qa_answers'][key] = []\n",
    "            for imp in qa[key]:\n",
    "                i['qa_tokens'][key].append(text_processing.tokenize(imp[0]))\n",
    "                i['qa_answers'][key].append(imp[1])\n",
    "        i.pop('qa_implications',None)\n",
    "    \n",
    "    else:\n",
    "        i['qa_tokens']={}\n",
    "        i['qa_answers']={}\n",
    "        for key in set(i['valid_answers']):\n",
    "            i['qa_tokens'][key] = [i['question_tokens']]\n",
    "            i['qa_answers'][key] = [key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(imdb,open(out_dir+'imdb_'+file+'2014.npy','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Ons for several restrictions in imdb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = numpy.load(out_dir+'imdb_'+file+'2014.npy',allow_pickle=True)\n",
    "imdb_ori = numpy.load(in_dir+'imdb_'+file+'2014.npy',allow_pickle=True)\n",
    "q = json.load(open('orig_data/vqa_v2.0/v2_mscoco_'+file+'2014_annotations.json','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and delete previous keys\n",
    "\n",
    "for i in imdb[1:]:\n",
    "    if i['is_imps']:\n",
    "        qa = i['qa_implications']\n",
    "        i['qa_tokens']={}\n",
    "        i['qa_answers']={}\n",
    "        for key in qa.keys():\n",
    "            i['qa_tokens'][key] = []\n",
    "            i['qa_answers'][key] = []\n",
    "            for imp in qa[key]:\n",
    "                i['qa_tokens'][key].append(text_processing.tokenize(imp[0]))\n",
    "                i['qa_answers'][key].append(imp[1])\n",
    "        i.pop('qa_implications',None)\n",
    "    \n",
    "    else:\n",
    "        i['qa_tokens']={}\n",
    "        i['qa_answers']={}\n",
    "        for key in set(i['valid_answers']):\n",
    "            i['qa_tokens'][key] = [i['question_tokens']]\n",
    "            i['qa_answers'][key] = [key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mode\n",
    "\n",
    "for i in imdb[1:]:\n",
    "    if not i['is_imps']:\n",
    "        for key in i['qa_answers']:\n",
    "            if key not in ['yes','no']:\n",
    "                i['qa_answers'][key] = [mode(i['valid_answers'])[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qmap = {}\n",
    "\n",
    "for ann in q['annotations']:\n",
    "    atype = ann['answer_type']\n",
    "    qid = ann['question_id']\n",
    "    qmap[qid] = atype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = []\n",
    "for i in range(1,len(imdb)):\n",
    "    if not imdb[i]['is_imps'] and qmap[imdb[i]['question_id']] != 'yes/no':\n",
    "        idx.append(i)\n",
    "\n",
    "imdb = numpy.delete(imdb,idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in imdb[1:]:\n",
    "    if not i['is_imps'] and qmap[i['question_id']]!='yes/no':\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete questions w/o any implications for all valid answers\n",
    "\n",
    "idx = []\n",
    "for i in range(1,len(imdb)):\n",
    "    if not imdb[i]['is_imps']:\n",
    "        idx.append(i)\n",
    "    imdb[i].pop('is_imps')\n",
    "\n",
    "imdb = numpy.delete(imdb,idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete questions w/o implications for any valid answers\n",
    "\n",
    "idx = []\n",
    "for i in range(1,len(imdb)):\n",
    "    qa = imdb[i]['qa_answers']\n",
    "    for key in qa.keys():\n",
    "        if len(qa[key])==0 :\n",
    "            idx.append(i)\n",
    "            break\n",
    "\n",
    "imdb = numpy.delete(imdb,idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(imdb,open(out_dir+'imdb_'+file+'2014.npy','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dir = 'data/imdb/'\n",
    "out_dir = 'data/imdb_imps/'\n",
    "file = 'train'\n",
    "\n",
    "imdb = numpy.load(out_dir+'imdb_'+file+'2014.npy',allow_pickle=True)\n",
    "imdb_ori = numpy.load(in_dir+'imdb_'+file+'2014.npy',allow_pickle=True)\n",
    "q = json.load(open('orig_data/vqa_v2.0/v2_mscoco_'+file+'2014_annotations.json','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_types = {}\n",
    "qmap = {}\n",
    "\n",
    "for ann in q['annotations']:\n",
    "    atype = ann['answer_type']\n",
    "    qid = ann['question_id']\n",
    "    qmap[qid] = atype\n",
    "    if atype not in question_types.keys():\n",
    "        question_types[atype] = []\n",
    "    question_types[atype].append(qid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Stats original vqa2.0')\n",
    "print('Total number of questions: %d' %(len(q['annotations'])))\n",
    "for key in question_types.keys():\n",
    "    print('%s lenght: %d percentage: %.2f' % (key,len(question_types[key]),100*len(question_types[key])/len(q['annotations'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_question_types = {}\n",
    "\n",
    "for ann in imdb[1:]:\n",
    "    qid = ann['question_id']\n",
    "    atype = qmap[qid]\n",
    "    \n",
    "    if atype not in updated_question_types.keys():\n",
    "        updated_question_types[atype]=[]\n",
    "    updated_question_types[atype].append(qid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Stats new dataset:')\n",
    "for key in updated_question_types:\n",
    "    print('%s lenght: %d percentage: %.2f' % (key,len(updated_question_types[key]),100*len(updated_question_types[key])/len(imdb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files = ['orig_data/vqa_v2.0/v2_OpenEnded_mscoco_train2014_questions.json',\n",
    "                'orig_data/vqa_v2.0/v2_OpenEnded_mscoco_val2014_questions.json',\n",
    "                'orig_data/vqa_v2.0/v2_OpenEnded_mscoco_test2015_questions.json']\n",
    "out_dir = '../'\n",
    "min_freq = 0\n",
    "vocab_file_name = 'vocabulary_vqa.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = Counter()\n",
    "questions = []\n",
    "\n",
    "for idx, input_file in enumerate(input_files):\n",
    "    with open(input_file, 'r') as f:\n",
    "        questions += json.load(f)['questions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18416"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_length = [None]*len(questions)\n",
    "for inx, question in enumerate(questions):\n",
    "    words = tokenize(question['question'])\n",
    "    question_length[inx] = len(words)\n",
    "    word_count.update(words)\n",
    "\n",
    "vocabulary = [w[0] for w in word_count.items() if w[1] >= min_freq]\n",
    "vocabulary.sort()\n",
    "vocabulary = ['<unk>'] + vocabulary\n",
    "\n",
    "len(vocabulary) #from original dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['train','val2train','minival'] #from implications\n",
    "questions_imps = []\n",
    "\n",
    "for file in files:\n",
    "    imps = pickle.load(open('data/imdb_imps/vqa_'+file+'_imps.pkl','rb'))\n",
    "    imdb_ori = numpy.load('data/imdb/imdb_'+file+'2014.npy',allow_pickle=True)\n",
    "\n",
    "    for i in imdb_ori[1:]:\n",
    "        key = i['question_id']\n",
    "        questions_imps += [q[0] for v in imps[key].values() for q in v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in questions_imps:\n",
    "    words = tokenize(question)\n",
    "    word_count.update(words)\n",
    "    \n",
    "vocabulary = [w[0] for w in word_count.items() if w[1] >= min_freq]\n",
    "vocabulary.sort()\n",
    "vocabulary = ['<unk>'] + vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1982279, 1105904)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(questions_imps),len(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = os.path.join(out_dir, vocab_file_name)\n",
    "with open(vocab_file, 'w') as f:\n",
    "    f.writelines([w+'\\n' for w in vocabulary])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18416"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>', '!', '!\"', '!.\"', '\"', '\"  \\'', '\" & \"', '\" \\'', '\" -', '\" <']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create implications imdb (for augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dir = 'data/imdb/'\n",
    "out_dir = 'data/imdb_imps/'\n",
    "file = 'val2train'\n",
    "\n",
    "imps = pickle.load(open(out_dir+'vqa_'+file+'_imps.pkl','rb'))\n",
    "imdb = numpy.load(out_dir+'imdb_'+file+'2014.npy',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_just_imps = [imdb[0].copy()]\n",
    "\n",
    "for i in imdb[1:]:\n",
    "    \n",
    "    if i['is_imps']:\n",
    "        vans = mode(i['valid_answers'])[0][0]\n",
    "\n",
    "        for q,a,implied in zip(i['qa_tokens'][vans],i['qa_answers'][vans],imps[i['question_id']][vans]):\n",
    "            cp = i.copy()\n",
    "            cp.pop('qa_tokens',None)\n",
    "            cp.pop('qa_answers',None)\n",
    "            cp.pop('is_imps',None)\n",
    "            cp['question_str'] = implied[0]\n",
    "            cp['question_tokens'] = q\n",
    "            cp['valid_answers'] = [a for _ in range(10)]\n",
    "            cp['all_answers'] = [a for _ in range(10)]\n",
    "            imdb_just_imps.append(cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(imdb_just_imps,open(in_dir+'imdb_just_imps'+file+'2014.npy','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(531092, 443758)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(imdb_just_imps),len(imdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'create_time': '2018-03-29 16:39',\n",
       " 'dataset_name': 'vqa',\n",
       " 'version': 1,\n",
       " 'has_answer': True,\n",
       " 'has_gt_layout': False}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_just_imps[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
